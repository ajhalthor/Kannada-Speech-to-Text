1. Create Dataset
	- Record Samples Sentences in `samples` directory.There are 100 such samples. Export them as WAV files with sample rate 44.1KHz. Be sure to choose a high sample rate as WAV files can be downsampled.

	- `prompts.txt` holds the transcription of those 100 sentences.

2. Create wordlist. 
	`wlist` consists of all unique words in `prompts.txt`. Create it with the following command using `bin/prompts2wlist.jl`.

		$ julia bin/prompts2wlist.jl prompts.txt wlist


3. Create Monophone Pronunciation Dictionary.
	- `monophones0` has all Kannada phones 46 with a silent phone `sil`.
	- `monophones1` has an additional short pause `sp` phone.

	The dictionary `dict` is constructed for every word in `wlist` using the following command.

		$ python python_scripts/create_dict.py

4. Word & Phone Transcription 
	Create Word Level Transcription File `words.mlf` from the `prompts.txt` file by separating each word line by line usnig `bin/prompts2mlf.jl`.

		$ julia bin/prompts2mlf.jl prompts.txt words.mlf

	Now Create a new file where every phone is on a new line using `mkphones0.led` and HLed.

		$ HLEd -A -D -T 1 -l '*' -d dict -i phones0.mlf mkphones0.led words.mlf 

	The phone level transcription is stored in `phones0.mlf`. We include short pause phones `sp` after after every word with the following command.

		$ HLEd -A -D -T 1 -l '*' -d dict -i phones1.mlf mkphones1.led words.mlf 

	This creates the required file with name `phones1.mlf`.

	NOTE : Contents of phones0.mlf and phones1.mlf  may not contain actual Kannada characters, but their **Octal Encding** instead. This is perfectly fine to workand we will convert it to Kannada Script at a later stage.

5. Convert Audio Data:
	The recorded Sentences in `samples` folder should be converted to PCM, mono-channel WAV files with a custom sample rate. First create an empty directory `samples_converted`. Open `convert_pcm_mono.py` and change the source and destination directory variable to `samples` and `samples_converted` respectively. Then execute.

		$ python python_scripts/convert_pcm_mono.py

	`samples_converted` will be our WAV data.


6. Create Training Paths
	`codetrain.scp` contains a list of  audio files on the left and corrusponding MFCC files on the right. Execute the following command to create `codetrain.scp`.

		$ python python_scripts/create_codetrain_scp.py


7. Create MFCC Vectors
	Create directory `train/mfcc` and specify a configuration file for the MFCCs to be created in `wav_config`. Execute the following :

		HCopy -A -D -T 1 -C wav_config -S codetrain.scp 

	This creates MFCCs for every sample. in `train/mfcc`.

	NOTE: Check the format of the MFCC feature vectors with HList.


8. Create a `train.scp` file which only contains the paths to the traning files. This is done by extracting the 2nd column of `codetrain.scp`. Hence we execute the following command to create `train.scp`.
	
	$ python python_scripts/create_train_scp.py


9. Initialize the HMM Prototype in `proto` and define a configuration file `config`.


10. Create an empty folder called `hmm0` and execute the HTK command HCompV as shown.
	
		$ HCompV -A -D -T 1 -C config -f 0.01 -m -S train.scp -M hmm0 proto

	This creates an `hmmdefs` and `vfloors` folder in the `hmm0` directory.


11. Copy `monophones0` into `hmm0` and create the hmmdefs file by getting into hmm0 directory and executing `create_hmmdefs.py`. 

	$ cd hmm0
	$ python create_hmmdefs.py

`hmmdefs` initializes all HMMs for every phone with parameters similar to the `proto` file.

12. Create a `macros` file by copying the first 3 lines of `proto` followed by the entire content of `vfloors`. The resulting file (for syl_16K) looks like this:
	```
		~o
		<STREAMINFO> 1 25
		<VECSIZE> 25<NULLD><MFCC_D_N_Z_0><DIAGC>
		~v varFloor1
		<Variance> 25
		 6.337364e-01 4.811290e-01 5.212138e-01 6.297672e-01 6.965734e-01 5.789758e-01 6.618909e-01 5.280533e-01 5.847340e-01 4.740644e-01 4.464558e-01 3.826577e-01 2.126088e-02 2.240220e-02 2.076481e-02 2.780402e-02 2.979447e-02 3.084975e-02 3.029560e-02 2.887547e-02 2.790046e-02 2.530517e-02 2.348650e-02 2.055844e-02 2.977529e-02

	```


13. Go to parent directory and create 15 folders and name them hmm0, hmm1 till hmm15.

	$ cd ..

14. Start Re-Estimation of Monophones using the following commands consecutively.

	$ HERest -A -D -T 1 -C config -I phones0.mlf -t 250.0 150.0 1000.0 -S train.scp -H hmm0/macros -H hmm0/hmmdefs -M hmm1 monophones0
	$ HERest -A -D -T 1 -C config -I phones0.mlf -t 250.0 150.0 1000.0 -S train.scp -H hmm1/macros -H hmm1/hmmdefs -M hmm2 monophones0
	$ HERest -A -D -T 1 -C config -I phones0.mlf -t 250.0 150.0 1000.0 -S train.scp -H hmm2/macros -H hmm2/hmmdefs -M hmm3 monophones0

	You may get warnings about the use of 3 phones : ೠ(ru), ಝ್(jha),  and ಞ್(nya). This is because they rarely occur in the dataset and in the langauge in general. Although ideally there should not be warnings, we continue due to dearth of these phones in contextual sentences. Do not worry. It does not affect our analysis. 

15. Create `sp` model.
	Copy contents of `hmm3` to `hmm4`. We add an HMM model for the short pause `sp` by copying the model for `sil`. Since this only has a single emitting state.
		- Since this only has a single emitting state remove the states 2 and 4 
		- rename state 3 to state 2.
		- the number of states reduces from 5 to 3. the first and last states are dummy states. Change <NUMSTATES> to 3.
		- The transition matrix now is size 3 x 3. Specify this dimension of the square matrix and use the probability matrix as shown.

			0.0 1.0 0.0
			0.0 0.9 0.1
			0.0 0.0 0.0

	Tie the center states of `sil` and `sp` and create a new set of models in `hmm5` with the following command.

		$ HHEd -A -D -T 1 -H hmm4/macros -H hmm4/hmmdefs -M hmm5 sil.hed monophones1

16. Perform Re-Estimation to incorperate updates.
	$ HERest -A -D -T 1 -C config  -I phones1.mlf -t 250.0 150.0 3000.0 -S train.scp -H hmm5/macros -H  hmm5/hmmdefs -M hmm6 monophones1
	% HERest -A -D -T 1 -C config  -I phones1.mlf -t 250.0 150.0 3000.0 -S train.scp -H hmm6/macros -H hmm6/hmmdefs -M hmm7 monophones1

17. We use `HVite` to align phone transcription and speech.

HVite -A -D -T 1 -l '*' -o SWT -b SENT-END -C config -H hmm7/macros -H hmm7/hmmdefs -i aligned.mlf -m -t 250.0 150.0 1000.0 -y lab -a -I words.mlf -S train.scp dict monophones1> HVite_log

The `aligned.mlf` is created and performs this mapping.

18. Perform Re-estimation

	$ HERest -A -D -T 1 -C config -I aligned.mlf -t 250.0 150.0 3000.0 -S train.scp -H hmm7/macros -H hmm7/hmmdefs -M hmm8 monophones1 
	$ HERest -A -D -T 1 -C config -I aligned.mlf -t 250.0 150.0 3000.0 -S train.scp -H hmm8/macros -H hmm8/hmmdefs -M hmm9 monophones1


This step generates the final set of HMMs in `hmm9` which is used for monophone analysis. However, we contonue with either Syllable Modeling or Triphone Modeling.

19. Create a Syllable Pronunciation Dictionary

	`create_dict_syl.py` is used to create the syllable dictionary `dict_syl.txt` and a syllable list `syllables1`. 

		$ python python_scripts/create_dict_syl.py

	The syllable dictionary has 3 fields for every word entry much like the monophone dictionary: internal representation, external representation and syllable sequence. 

20. Convert Syllables to triphone representation

	Execute `create_dict_syl_tri.py` to get the triphone sequence for every syllable. Note, this is not the same as triphone ASR where we obtain triphones for words. 

		python python_scripts/create_dict_syl_tri.py

	Triphone representation of syllables is used because our decoder, HVite, can only work with triphones inherently. The resulting dictionary is `dict-tri`. All syllables (of trihpones) encountered are stored in `fulllist0`


21. 

	`fulllist0` consists of 554 unique syllables used across the training samples, including short pause `sp` and silence `sil`. We want to append any additional entries in the `monophones0` file that may have been left out of the samples. 

	ಘ್ (gha)
	ಛ್ (cha)
	ಝ್ (jha)
	ಞ್ (nya)
	ಠ್ (ta)
	ಢ್ (da)
	ಥ್ (tha)
	ಭ್ (ba)
	ೠ	(ru)

	These are appended to `fulllist0` and the entire list is written to a new file `fulllist`. This is done with the following command.

		$ julia bin/fixfulllist.jl fulllist0 monophones0 fulllist


22. Create syllabic transcriptions (using triphones)
	Execute `create_wintri.py` create the syllable transcription of training sample sentences `prompts.txt`.

		python python_scripts/create_wintri.py

	This creates a master label file `wintri.mlf` with syllabic transcriptions using triphones.

23. Tie HMMs 

	Create `mktri.hed` to tie HMMs together so they share the same parameters by executing the following commands.

		$ julia bin/mktrihed.jl monophones1 triphones1 mktri.hed

		$ HHEd -A -D -T 1 -H hmm9/macros -H hmm9/hmmdefs -M hmm10 mktri.hed monophones1 

24. Re-Estimate the Tied parameters with 2 steps of training:

		
		$ HERest  -A -D -T 1 -C config -I wintri.mlf -t 250.0 150.0 3000.0 -S train.scp -H hmm10/macros -H hmm10/hmmdefs -M hmm11 triphones1 
		$ HERest  -A -D -T 1 -C config -I wintri.mlf -t 250.0 150.0 3000.0 -s stats -S train.scp -H hmm11/macros -H hmm11/hmmdefs -M hmm12 triphones1 
	
	A number of warnings occur due to few examples of Syllables. We solve this by tying HMMs for different syllables .


25. Construction of descision tree

	Based on contextual questions for the Kannada langauge specified in `tree1.hed`, a decision tree is created. This is used to tie trihpones (of syllables) with similar context, reducing the number of HMM models.

	Copy contents to a file `tree.hed`.

		$ cat tree1.hed > tree.hed

	Execute `mkclscript.jl` to append state clusters at the end of the newly created `tree.hed`.

		$ julia bin/mkclscript.jl monophones0 tree.hed

26.

		$ HHEd -A -D -T 1 -H hmm12/macros -H hmm12/hmmdefs -M hmm13 tree.hed triphones1 

	You may get an error : "FindProtoModel: no proto for ೠ in hSet". This appears because ೠ is a part of our monophones file, yet there are no examples using this character. To get rid of this error, remove ೠ from `fulllist` near the last line of the file and execute this HHed command once more. 

	This creates a `tiedlist` file which has syllables that have the same pronunciation in different context on a line.

27.  Perform re-estimation 2 more times to generate the final hmm definitions in `hmm15/hmmdefs`.

		$ HERest -A -D -T 1 -T 1 -C config -I wintri.mlf  -t 250.0 150.0 3000.0 -S train.scp -H hmm13/macros -H hmm13/hmmdefs -M hmm14 tiedlist
		$ HERest -A -D -T 1 -T 1 -C config -I wintri.mlf  -t 250.0 150.0 3000.0 -S train.scp -H hmm14/macros -H hmm14/hmmdefs -M hmm15 tiedlist


28. 

	`hmm15/hmmdefs` and `tiedlist` are octal encoded. Convert them to Kannada text by opening the python interpreter in the terminal, assigning the entire file contents to a variable and subsequently printing the variable. Consider copying the contents of `tiedlist` into variable `a` as shown. 

		$ python
		>>> a = """
		... \340\262\206+\340\262\256\340\263\215
		...\340\262\247\340\263\215-\340\262\205
		...\340\262\247\340\263\215-\340\262\206
		...\340\262\247\340\263\215-\340\262\207
		.
		.
		.
		"""
		>>>
		>>> print a
		ಆ+ಮ್
		ಧ್-ಅ
		ಧ್-ಆ
		ಧ್-ಇ
		.
		.
		.
		>>>

	Copy result of `print a` and paste it into `tiedlist_kn`. Repeat the same for `hmm15/hmmdef`. Copy the result of say `print b` into `hmm15/hmmdefs_kn`. `b` is the variable holding the contents of `hmm15/hmmdefs`.




Testing and Evaluation:

29. Convert Testing Data:
	Recorded Voice Samples for every word is stored as WAV files in `recorded_test_samples` at a sample rate 44.1 kHz.

	Create an empty directory `test_samples`. Open `convert_pcm_mono.py` and change the source and destination directory variable to `recorded_test_samples` and `test_samples` respectively. Then execute

		$ python python_scripts/convert_pcm_mono.py

	`test_samples` will consist of our Mono-channel PCM WAV test files.


30. Create Grammar File

	The grammar of recognized sentences is defined in `grammar`. For the problem of isolated word recognition, a "sentence" is a "word" flanked by 2 slience phones. 

		$ HParse grammar wdnet

	This will create `wdnet`.

31. Create Test Reference

	The test reference file `test_ref.mlf` has a list of testing files and corresponding word to be recognized. It is created using `create_testref_mlf.py` by executing the following command.

		$ python python_scripts/create_testref_mlf.py

	You may need to replace `SENT-START` and `SENT-END` with `sil`. A sample part of the file should look like the following:

		"*/ಅಗಾಧ_.lab"
		sil
		ಅಗಾಧ
		sil
		.
		"*/ಅತಿ_.lab"
		sil
		ಅತಿ
		sil
		.
		"*/ಅತ್ಯದ್ಭುತ_.lab"
		sil
		ಅತ್ಯದ್ಭುತ
		sil
		.
		"*/ಅಥವಾ_.lab"
		sil
		ಅಥವಾ
		sil
		.
		"*/ಅದರ_.lab"
		sil
		ಅದರ
		sil
		.
		"*/ಅದು_.lab"
		sil
		ಅದು
		sil
		.
		"*/ಅದೇ_.lab"
		sil
		ಅದೇ
		sil

32. Recognize Test Samples
	
	`recount.mlf` consists of the predictions of our ASR. This is generated by running our decoder HVite as follows:

		$ Hvite -C config_Hvite  -H hmm15/macros -H hmm15/hmmdefs_kn -S test.scp -i recount.mlf -p 0.0 -s 5.0  -w wdnet -y rec dict-tri tiedlist_kn

	Some changes are required in the `recount.mlf` file:
		- replace `test_samples` in the path of every `rec` file with an asterisk `*`
		- Replace `SENT-START` and `SENT-END` with `sil`


	The file should look as shown below:

		"*/ಅದೇ_.rec"
		0 4000000 sil -1541.416870
		4000000 9700000 \340\262\205\340\262\246\340\263\207 -3166.399170
		9700000 15400000 sil -2632.300781
		.
		"*/ಅಧಿಕೃತ_.rec"
		0 3300000 sil -1489.629883
		3300000 12900000 \340\262\205\340\262\247\340\262\277\340\262\225\340\263\203\340\262\244 -5682.479492
		12900000 14500000 sil -737.820129
		.
		"*/ಅಧ್ಯಕ್ಷೀಯ_.rec"
		0 3600000 sil -1499.818726
		3600000 15300000 \340\262\205\340\262\247\340\263\215\340\262\257\340\262\225\340\263\215\340\262\267\340\263\200\340\262\257 -6673.943848
		15300000 23400000 sil -3269.386475
		.

33. Get results

	Get the results of analtsis using `HResult` as shown. 

		$ HResults -t -I testref.mlf tiedlist_kn recount.mlf

	The option `-t` allows us to additionally see the words that were incorrectly predicted. 



SMALL DICTIONARY ANALYSIS


Instead of the 750 word dictionaries, we also perform analysis for 500 word dictionaries. They are split into 3 separate dictionaries based on word length.

In `syl_16K`, go to the `small_syl` directory.

	$ cd small_syl 


Create the following directories:
1. short
	mono
	syl
2. middle
	mono
	syl
3. long
	mono
	syl
4. syllable_resources

Copy the `hmm15` directory and `tiedlist_kn` file into the newly created `syllable_resources` directory.

Create the syllable dictionaries by executing the following python file.

	$ python create_small_dict_syl.py

This creates the following 6 text files:
small_syl
	short
		syl
			dict_short.txt
			wlist_short.txt
	middle
		syl
			dict_middle.txt
			wlist_middle.txt
	long
		syl
			dict_long.txt
			wlist_long.txt

Similarly create the monophone dictionaries by executing the following.

	$ python create_small_dictionary.py

This creates the following 6 text files:
small_syl
	short
		mono
			dict_short.txt
			wlist_short.txt
	middle
		mono
			dict_middle.txt
			wlist_middle.txt
	long
		mono
			dict_long.txt
			wlist_long.txt


From now onwords, we need every combination of subword (mono, syl) with every dictionary (short, middle, long)

In `create_small_grammar.py`, make sure the folder and subword are initially 'short' and 'mono'. The beginnning of the file should look like the following : 
```
folder = 'short'
subword = 'mono'
```

Now execute the following.

	$ python create_small_grammar.py

This will create the grammar file in the following directory structure:

small_syl
	short
		mono
			grammar_short




In `create_small_test_scp.py` make sure the folder and subword are 'short' and 'mono' respectively as was the case with the previous file. 

We then execute the following command.
	
	$ python create_small_test_scp.py

This creates the following scp file.

small_syl
	short
		mono
			test_short.scp




In `create_small_testref_mlf.py` we repeat the same. Make sure folder and subword are 'short' and 'mono' respectively and execute the following command.

	$ python create_small_testref_mlf.py

This creates the `testref` file.

small_syl
	short
		mono
			testref_short.mlf


NOTE : Changing `short` and `mono` in the code will create the corresponding file.

Repeat the steps , ,and by replacing 'short' with 'middle' and 'long' and replacing 'mono' with 'syl'. We should end up with 6 combinations.


Perform Testing and evaluation using steps 29 to 33. Enure the full path is used
E.g. syl/long/grammar_long

For the tied list, use syllable_resources/tiedlist_kn
For hmmdefs, use syllable_resources/hmmdefs_kn.


You should be good to go! 


